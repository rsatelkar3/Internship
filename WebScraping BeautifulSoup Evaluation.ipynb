{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db65fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59a89c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name  Rating  Year\n",
      "0                     Ship of Theseus     8.0  2012\n",
      "1                              Iruvar     8.4  1997\n",
      "2                     Kaagaz Ke Phool     7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India     8.1  2001\n",
      "4                     Pather Panchali     8.2  1955\n",
      "..                                ...     ...   ...\n",
      "95                        Apur Sansar     8.4  1959\n",
      "96                        Kanchivaram     8.2  2008\n",
      "97                    Monsoon Wedding     7.3  2001\n",
      "98                              Black     8.1  2005\n",
      "99                            Deewaar     8.0  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "movies = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "names = []\n",
    "ratings = []\n",
    "years = []\n",
    "\n",
    "for movie in movies:\n",
    "   \n",
    "    name = movie.find('a').text.strip()\n",
    "    names.append(name)\n",
    "    \n",
    "   \n",
    "    rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    ratings.append(float(rating))\n",
    "    \n",
    "    \n",
    "    year = movie.find('span', class_='lister-item-year').text.strip('()')\n",
    "    years.append(year)\n",
    "\n",
    "\n",
    "data = {'Name': names, 'Rating': ratings, 'Year': years}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b86e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Product Name, Price, Discount]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://peachmode.com/search?q=bags'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "products = soup.find_all('div', class_='product-list-item')\n",
    "\n",
    "names = []\n",
    "prices = []\n",
    "discounts = []\n",
    "\n",
    "\n",
    "for product in products:\n",
    "   \n",
    "    name = product.find('h3', class_='product-name').text.strip()\n",
    "    names.append(name)\n",
    "    \n",
    "\n",
    "    price = product.find('span', class_='price').text.strip()\n",
    "    prices.append(price)\n",
    "    \n",
    "\n",
    "    discount_tag = product.find('span', class_='discount-percentage')\n",
    "    if discount_tag:\n",
    "        discount = discount_tag.text.strip()\n",
    "    else:\n",
    "        discount = \"No discount\"\n",
    "    discounts.append(discount)\n",
    "\n",
    "\n",
    "data = {'Product Name': names, 'Price': prices, 'Discount': discounts}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52677baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men’s cricket:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 55\u001b[0m\n\u001b[0;32m     51\u001b[0m url_bowlers \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 ODI teams in men’s cricket:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m df_teams \u001b[38;5;241m=\u001b[39m scrape_icc_rankings(url_teams)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_teams\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36mscrape_icc_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     18\u001b[0m teams \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m ratings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 22\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[0;32m     27\u001b[0m     name \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu-hide-phablet\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrape_icc_rankings(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    " \n",
    "    table = soup.find('table', class_='table rankings-table')\n",
    "\n",
    "   \n",
    "    names = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "\n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    \n",
    "    for row in rows:\n",
    " \n",
    "        name = row.find('span', class_='u-hide-phablet').text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "       \n",
    "        rating = row.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "      \n",
    "        team = row.find('span', class_='u-hide-mobile').text.strip()\n",
    "        teams.append(team)\n",
    "\n",
    "  \n",
    "    data = {'Name': names, 'Team': teams, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "url_teams = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "\n",
    "\n",
    "url_batsmen = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "\n",
    "\n",
    "url_bowlers = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "\n",
    "\n",
    "print(\"Top 10 ODI teams in men’s cricket:\")\n",
    "df_teams = scrape_icc_rankings(url_teams)\n",
    "print(df_teams.head(10))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Top 10 ODI batsmen:\")\n",
    "df_batsmen = scrape_icc_rankings(url_batsmen)\n",
    "print(df_batsmen.head(10))\n",
    "print()\n",
    "\n",
    "print(\"Top 10 ODI bowlers:\")\n",
    "df_bowlers = scrape_icc_rankings(url_bowlers)\n",
    "print(df_bowlers.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b710e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men’s cricket:\n",
      "No table found on https://www.icc-cricket.com/rankings/mens/player-rankings/test\n",
      "Failed to fetch data for top 10 ODI teams.\n",
      "\n",
      "Top 10 ODI batsmen:\n",
      "No table found on https://www.icc-cricket.com/rankings/mens/player-rankings/test\n",
      "Failed to fetch data for top 10 ODI batsmen.\n",
      "\n",
      "Top 10 ODI bowlers:\n",
      "No table found on https://www.icc-cricket.com/rankings/mens/player-rankings/test\n",
      "Failed to fetch data for top 10 ODI bowlers.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrape_icc_rankings(url):\n",
    "   \n",
    "    response = requests.get(url)\n",
    "    \n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "   \n",
    "    if table is None:\n",
    "        print(f\"No table found on {url}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    names = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "  \n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "  \n",
    "    for row in rows:\n",
    "        \n",
    "        name = row.find('td', class_='rankings-block__banner--team-name').text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "  \n",
    "        rating = row.find('td', class_='rankings-block__banner--rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "        \n",
    "        team = row.find('span', class_='u-hide-phablet').text.strip()\n",
    "        teams.append(team)\n",
    "\n",
    "  \n",
    "    data = {'Name': names, 'Team': teams, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "url_teams = 'https://www.icc-cricket.com/rankings/mens/player-rankings/test'\n",
    "\n",
    "\n",
    "url_batsmen = 'https://www.icc-cricket.com/rankings/mens/player-rankings/test'\n",
    "\n",
    "\n",
    "url_bowlers = 'https://www.icc-cricket.com/rankings/mens/player-rankings/test'\n",
    "\n",
    "\n",
    "print(\"Top 10 ODI teams in men’s cricket:\")\n",
    "df_teams = scrape_icc_rankings(url_teams)\n",
    "if df_teams is not None:\n",
    "    print(df_teams.head(10))\n",
    "else:\n",
    "    print(\"Failed to fetch data for top 10 ODI teams.\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Top 10 ODI batsmen:\")\n",
    "df_batsmen = scrape_icc_rankings(url_batsmen)\n",
    "if df_batsmen is not None:\n",
    "    print(df_batsmen.head(10))\n",
    "else:\n",
    "    print(\"Failed to fetch data for top 10 ODI batsmen.\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Top 10 ODI bowlers:\")\n",
    "df_bowlers = scrape_icc_rankings(url_bowlers)\n",
    "if df_bowlers is not None:\n",
    "    print(df_bowlers.head(10))\n",
    "else:\n",
    "    print(\"Failed to fetch data for top 10 ODI bowlers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a2f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "posts = soup.find_all('div', class_='post-card-content')\n",
    "\n",
    "\n",
    "for post in posts:\n",
    "\n",
    "    heading = post.find('h2', class_='post-card__title').text.strip()\n",
    "\n",
    "    date = post.find('time', class_='post-card__published-at')['datetime']\n",
    "    \n",
    "   \n",
    "    content = post.find('div', class_='post-card__excerpt').text.strip()\n",
    "    \n",
    "\n",
    "    youtube_video = post.find('div', class_='patreon-post-youtube').find('iframe')\n",
    "    if youtube_video:\n",
    "        youtube_video_url = youtube_video['src']\n",
    "        \n",
    "        video_id = youtube_video_url.split('/')[-1]\n",
    "        likes = get_youtube_likes(video_id)\n",
    "    else:\n",
    "        likes = 'N/A'\n",
    "    \n",
    "    # Print the extracted information\n",
    "    print(\"Heading:\", heading)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Content:\", content)\n",
    "    print(\"Likes:\", likes)\n",
    "    print()\n",
    "\n",
    "\n",
    "def get_youtube_likes(video_id):\n",
    "    # Construct the YouTube API URL\n",
    "    api_url = f'https://www.googleapis.com/youtube/v3/videos?part=statistics&id={video_id}&key=YOUR_YOUTUBE_API_KEY'\n",
    "    \n",
    "  \n",
    "    response = requests.get(api_url)\n",
    "    data = response.json()\n",
    "    \n",
    "\n",
    "    if 'items' in data and data['items']:\n",
    "        likes = data['items'][0]['statistics']['likeCount']\n",
    "        return likes\n",
    "    else:\n",
    "        return 'N/A'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6137c77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House details for Indira-nagar:\n",
      "\n",
      "House details for Jayanagar:\n",
      "\n",
      "House details for Rajaji-nagar:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_house_details(locality):\n",
    "   \n",
    "    url = f'https://www.nobroker.in/property/sale/{locality}/?searchParam=W3sibGF0IjoxMy4wNDUyMTQ1LCJsb24iOjc3Ljg4NDU3ODcsInBsYWNlSWQiOiJDaElKcEZmSTdfUHN2enNSQmNTbGcyS3dWYlEiLCJwbGFjZU5hbWUiOiJJbmRpcmEgTmFnYXIifV0=&radius=2.0'\n",
    "    \n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "\n",
    "    houses = soup.find_all('div', class_='nb__2JHKO')\n",
    "    \n",
    " \n",
    "    house_details = []\n",
    "    \n",
    "\n",
    "    for house in houses:\n",
    "      \n",
    "        title = house.find('h2', class_='heading-6 font-semi-bold nb__1AShY').text.strip()\n",
    "        \n",
    "  \n",
    "        location = house.find('div', class_='nb__2CMjv').text.strip()\n",
    "        \n",
    "  \n",
    "        area = house.find('div', class_='nb__3oNyC').text.strip()\n",
    "        \n",
    "     \n",
    "        emi = house.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "        \n",
    "   \n",
    "        price = house.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "        \n",
    "       \n",
    "        house_details.append({\n",
    "            'Title': title,\n",
    "            'Location': location,\n",
    "            'Area': area,\n",
    "            'EMI': emi,\n",
    "            'Price': price\n",
    "        })\n",
    "    \n",
    "    return house_details\n",
    "\n",
    "\n",
    "localities = ['indira-nagar', 'jayanagar', 'rajaji-nagar']\n",
    "\n",
    "\n",
    "for locality in localities:\n",
    "    print(f\"House details for {locality.capitalize()}:\")\n",
    "    house_details = scrape_house_details(locality)\n",
    "    for house in house_details:\n",
    "        print(house)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f5a410",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 41\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m product_details\n\u001b[0;32m     38\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.bewakoof.com/bestseller?sort=popular\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 41\u001b[0m product_details \u001b[38;5;241m=\u001b[39m scrape_product_details(url)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, product \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(product_details, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36mscrape_product_details\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     16\u001b[0m product_details \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m products[:\u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m---> 21\u001b[0m     name \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     24\u001b[0m     price \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     27\u001b[0m     image_url \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_product_details(url):\n",
    "   \n",
    "    response = requests.get(url)\n",
    "    \n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "  \n",
    "    products = soup.find_all('div', class_='productCardBox')\n",
    "    \n",
    "  \n",
    "    product_details = []\n",
    "    \n",
    " \n",
    "    for product in products[:10]:\n",
    "   \n",
    "        name = product.find('h3', class_='name').text.strip()\n",
    "        \n",
    "  \n",
    "        price = product.find('span', class_='price').text.strip()\n",
    "        \n",
    "        \n",
    "        image_url = product.find('img')['src']\n",
    "        \n",
    "  \n",
    "        product_details.append({\n",
    "            'Name': name,\n",
    "            'Price': price,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "\n",
    "product_details = scrape_product_details(url)\n",
    "\n",
    "\n",
    "for index, product in enumerate(product_details, start=1):\n",
    "    print(f\"Product {index}:\")\n",
    "    print(\"Name:\", product['Name'])\n",
    "    print(\"Price:\", product['Price'])\n",
    "    print(\"Image URL:\", product['Image URL'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "882677c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 1:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-blue-round-in-bugs-graphic-printed-oversized-short-top-589789-1707221362-1.jpg\n",
      "\n",
      "Product 2:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/women-aop-regular-pyjama-604946-1702644020-1.jpg\n",
      "\n",
      "Product 3:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-save-our-home-t-shirt-276160-1705996290-1.jpg\n",
      "\n",
      "Product 4:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-grey-slim-fit-joggers-321349-1684237833-1.jpg\n",
      "\n",
      "Product 5:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-white-t-shirt-105-1702967985-1.jpg\n",
      "\n",
      "Product 6:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/women-aop-oversize-t-shirt-14-580373-1686300320-1.jpg\n",
      "\n",
      "Product 7:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-purple-billionaire-girls-club-graphic-printed-oversized-t-shirt-604125-1708349620-1.jpg\n",
      "\n",
      "Product 8:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-oversized-vest-500149-1671022126-1.jpg\n",
      "\n",
      "Product 9:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-pink-avoiding-responsibilities-graphic-printed-t-shirt-585778-1682002484-1.jpg\n",
      "\n",
      "Product 10:\n",
      "Name: N/A\n",
      "Price: N/A\n",
      "Image URL: https://images.bewakoof.com/t640/whatever-cat-boyfriend-t-shirt-388114-1678261106-1.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_product_details(url):\n",
    "  \n",
    "    response = requests.get(url)\n",
    "    \n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('div', class_='productCardBox')\n",
    "    \n",
    "   \n",
    "    product_details = []\n",
    "    \n",
    " \n",
    "    for product in products[:10]:\n",
    "      \n",
    "        name_elem = product.find('h3', class_='name')\n",
    "        name = name_elem.text.strip() if name_elem else 'N/A'\n",
    "        \n",
    "        \n",
    "        price_elem = product.find('span', class_='price')\n",
    "        price = price_elem.text.strip() if price_elem else 'N/A'\n",
    "        \n",
    "        \n",
    "        image_url_elem = product.find('img')\n",
    "        image_url = image_url_elem['src'] if image_url_elem else 'N/A'\n",
    "        \n",
    "\n",
    "        product_details.append({\n",
    "            'Name': name,\n",
    "            'Price': price,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "\n",
    "product_details = scrape_product_details(url)\n",
    "\n",
    "\n",
    "for index, product in enumerate(product_details, start=1):\n",
    "    print(f\"Product {index}:\")\n",
    "    print(\"Name:\", product['Name'])\n",
    "    print(\"Price:\", product['Price'])\n",
    "    print(\"Image URL:\", product['Image URL'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcf447ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_news_details(url):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "   \n",
    "    news_items = soup.find_all('div', class_='Card-title')\n",
    "    \n",
    "    headings = []\n",
    "    dates = []\n",
    "    news_links = []\n",
    "    \n",
    "    for news_item in news_items:\n",
    "      \n",
    "        heading = news_item.find('a').text.strip()\n",
    "        headings.append(heading)\n",
    "        \n",
    "    \n",
    "        date = news_item.find_next('time').text.strip()\n",
    "        dates.append(date)\n",
    "        \n",
    "    \n",
    "        news_link = 'https://www.cnbc.com' + news_item.find('a')['href']\n",
    "        news_links.append(news_link)\n",
    "    \n",
    "    return headings, dates, news_links\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "headings, dates, news_links = scrape_news_details(url)\n",
    "\n",
    "for i in range(len(headings)):\n",
    "    print(\"Heading:\", headings[i])\n",
    "    print(\"Date:\", dates[i])\n",
    "    print(\"News Link:\", news_links[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17ed0faf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:7\u001b[1;36m\u001b[0m\n\u001b[1;33m    soup = BeautifulSoup(response.text, 'html.parser')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_news_details(url):\n",
    "      response = requests.get(url)\n",
    "    \n",
    "     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "      news_items = soup.find_all('div', class_='Card-textContainer')\n",
    "    \n",
    "    headings = []\n",
    "    dates = []\n",
    "    news_links = []\n",
    "    \n",
    "       for news_item in news_items:\n",
    "     \n",
    "        heading = news_item.find('a').text.strip()\n",
    "        headings.append(heading)\n",
    "        \n",
    "    \n",
    "        date_elem = news_item.find('time')\n",
    "        date = date_elem.text.strip() if date_elem else 'N/A'\n",
    "        dates.append(date)\n",
    "        \n",
    "        news_link = 'https://www.cnbc.com' + news_item.find('a')['href']\n",
    "        news_links.append(news_link)\n",
    "    \n",
    "    return headings, dates, news_links\n",
    "\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "\n",
    "headings, dates, news_links = scrape_news_details(url)\n",
    "\n",
    "for i in range(len(headings)):\n",
    "    print(\"Heading:\", headings[i])\n",
    "    print(\"Date:\", dates[i])\n",
    "    print(\"News Link:\", news_links[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ab794e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:18\u001b[1;36m\u001b[0m\n\u001b[1;33m    headings.append(heading)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_news_details(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    news_items = soup.find_all('div', class_='Card')\n",
    "    \n",
    "     headings = []\n",
    "    dates = []\n",
    "    news_links = []\n",
    "    \n",
    "       for news_item in news_items:\n",
    "             heading = news_item.find('a', class_='Card-titleLink').text.strip()\n",
    "        headings.append(heading)\n",
    "        \n",
    "        date_elem = news_item.find('time')\n",
    "        date = date_elem.text.strip() if date_elem else 'N/A'\n",
    "        dates.append(date)\n",
    "        \n",
    "              news_link = 'https://www.cnbc.com' + news_item.find('a')['href']\n",
    "        news_links.append(news_link)\n",
    "    \n",
    "    return headings, dates, news_links\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region\n",
    "\n",
    "headings, dates, news_links = scrape_news_details(url)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(headings)):\n",
    "    print(\"Heading:\", headings[i])\n",
    "    print(\"Date:\", dates[i])\n",
    "    print(\"News Link:\", news_links[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "265e98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape news details\n",
    "def scrape_news_details(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all news items\n",
    "    news_items = soup.find_all('div', class_='Card')\n",
    "    \n",
    "    # Lists to store data\n",
    "    headings = []\n",
    "    dates = []\n",
    "    news_links = []\n",
    "    \n",
    "    # Iterate through each news item and extract required information\n",
    "    for news_item in news_items:\n",
    "        # Extract heading\n",
    "        heading = news_item.find('a', class_='Card-titleLink').text.strip()\n",
    "        headings.append(heading)\n",
    "        \n",
    "        # Extract date\n",
    "        date_elem = news_item.find('time')\n",
    "        date = date_elem.text.strip() if date_elem else 'N/A'\n",
    "        dates.append(date)\n",
    "        \n",
    "        # Extract news link\n",
    "        news_link = 'https://www.cnbc.com' + news_item.find('a')['href']\n",
    "        news_links.append(news_link)\n",
    "    \n",
    "    return headings, dates, news_links\n",
    "\n",
    "# URL of the website\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Scrape news details\n",
    "headings, dates, news_links = scrape_news_details(url)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(headings)):\n",
    "    print(\"Heading:\", headings[i])\n",
    "    print(\"Date:\", dates[i])\n",
    "    print(\"News Link:\", news_links[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "527bbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape paper details\n",
    "def scrape_paper_details(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all paper items\n",
    "    paper_items = soup.find_all('div', class_='article-details')\n",
    "    \n",
    "    # Lists to store data\n",
    "    titles = []\n",
    "    dates = []\n",
    "    authors = []\n",
    "    \n",
    "    # Iterate through each paper item and extract required information\n",
    "    for paper_item in paper_items:\n",
    "        # Extract title\n",
    "        title = paper_item.find('h3', class_='article-title').text.strip()\n",
    "        titles.append(title)\n",
    "        \n",
    "        # Extract date\n",
    "        date = paper_item.find('span', class_='date').text.strip()\n",
    "        dates.append(date)\n",
    "        \n",
    "        # Extract author\n",
    "        author = paper_item.find('span', class_='name').text.strip()\n",
    "        authors.append(author)\n",
    "    \n",
    "    return titles, dates, authors\n",
    "\n",
    "# URL of the website\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "\n",
    "# Scrape paper details\n",
    "titles, dates, authors = scrape_paper_details(url)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(titles)):\n",
    "    print(\"Title:\", titles[i])\n",
    "    print(\"Date:\", dates[i])\n",
    "    print(\"Author:\", authors[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d0a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
